---
title: "Lab 4"
subtitle: "w203: Statistics for Data Science"
author: "Gurdit Chahal"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### Introduction

There is a political campaign that is set out to understand source factors of crime in order to develop policy suggestions that can possibly be applied to local government. In order to make data-informed decisions, our group has been hired to provide research for the political campaign and has been given a dataset of crime statistics for a set of counties along with a codebook explaining the variables. Using EDA, hypothesis testing, and our knowledge of linear regression, we look to examine and possibly use available factors such as wage and police per capita (independent variables) to help understand their impact on crimes per person (dependent variable) as well as trying to help address the campaign's questions from a causal lens. We will also explain limitations and assumptions behind our choices of analysis and modeling and our results.
  
```{r}
library(car)
# setwd("~/Berkeley/W203 Statistics/Assignments/Lab_4") # MP Path #
setwd("C:/Users/AOlson/Documents/UC Berkeley MIDS/W203 Statistics/Lab 4/Lab_4")
crime <- read.csv(file='crime_v2_updated.csv',header=TRUE,sep=',')

```

  Here we examine the variables. 
  
  
```{r}
head(crime)
summary(crime)
str(crime)
nrow(crime)
apply(!is.na(crime), MARGIN = 2, mean)
```
Don't see any immediate na's. Hence we have to be more careful in finding anomolies/missing values in the way that they are coded.
We note county doesnt mean anything in terms of descriptive statistics... We might use this id to see if there's random sampling or not. These counties might all be clustered in some fashion (as local government naturally would...). Year is 88 accross the data (1988) so it is truly cross sectional/true for that time period. 

We note west, central and urban as indicator values. West and central have an implicit base value of [0,0] to indicate the eastern part of the state. Note ymales is a proprotion and is for a certain age group, can't explicitly look for a comment on females but rather have to generalize of population outside this particular group.

Anomalies/errors: probsen and probconv are well above 1 in terms of max which violates them being "true" probabilities. Might make more sense to consider these as ordinal. If there are relatively few such points, might be able to discard as anomalies.

```{r}
nrow(subset(crime,(probsen > 1) | (probconv > 1)))

inspect_prob <- subset(crime,(probsen > 1) | (probconv > 1))

```
We see that 10 of the 90 observations actually break the definition of probability. This is significant as that is 11% of our observations. Only one of these is violates from probsen and it happens to coincide with probconv. There don't seem to be any inconsistensies in the other columns or trends . We could consider breaking our data into quantiles and treating it as subjective/ordinal data. This will likely be the case for all three of the probabilities since they seem subjective. As a simple first transform, .5 could be coded as unsure with less being sure of the even not happening and above as being more confident that the event is happening. What would be interesting to explore is the relationship of probability of sentence and sentence length along with probability of conviction and arrest. Would also wanna check the group/joint significance of these. (Note: We may not actually need these two variables, since we are focused on finding the "determinants of crime". Let's keep this in mind before removing the records)



```{r}
for (i in 4:26)
  {
 hist(crime[,i],main=" ",xlab=NULL, breaks=20)
  title(names(crime)[i], line = -1)
}
nrow(subset(crime,crime <.02))
```

We first take note of our dependent variable of crime and see a positive skew (Some counties seem to have much more crime than the average). We note that all the values are already between 0 and .1 (non-inclusive). Tax also has a strong positive skew and is strictly positive here (elasticity here also makes sense). (Density has a large skew also. Cities will be much more populated than rural areas) Ymale and mix also seem to fall in that potential category. The wage variables seem natural to apply log transformations although they don't seem far from normal so we might want to compare/analyze further. (Note: Typically it makes sense to apply log to wage when you have a group with diverse jobs, since naturally some jobs make significantly more than others. However, since these are split into groups with similar jobs we see more of a normal distribution for each group. May not need to take log here.) We might also want to look at the group/joint effect of wage as well. Other variables to consider might be ratio of min wage to max.

```{r}
# Create new dataframe with applied transformations and move forward with this?
crime_transformed <- crime
crime_transformed$crime <- log(crime_transformed$crime)
crime_transformed$probconv <- log(crime_transformed$probconv)
crime_transformed$police <- log(crime_transformed$police)
crime_transformed$density <- log(crime_transformed$density)
crime_transformed$tax <- log(crime_transformed$tax)
crime_transformed$ymale <- log(crime_transformed$ymale)

# Now are variables are closer to normally distributed
for (i in 4:26)
  {
 hist(crime_transformed[,i],main=" ",xlab=NULL, breaks=20)
  title(names(crime_transformed)[i], line = -1)
}

```


```{r}
# install.packages("corrplot")
library(corrplot)
cor(crime) #quite a few variables, might wanna consider correlation heatmap here
corrplot(cor(crime_transformed[c(4:26)]), method = 'square', type = 'upper', tl.col = 'black')


```
Looking at the top line of the correlation matrix (crime) we can see that the highest positively correlated varaibles are (density, urban, federal employee wage - weekly). The strongest negatively correlated variables are (probability of arrest, probability of prison sentance). For linear regression, we cannot use a 'perfectly correlated' variable set. Induitively density and urban are highly correlated and it is doubtful whether or not additional information related to 'crime' would be included in one variable over the other:

```{r}
cor(crime$urban, crime$density)
```

Oddly the probability of arrest or prison is negatively correlated with crime. There is no information related to how these statistics were computed so it is difficult to understand why they would be negatively correlated, when intuitively a higher probability of arrest/prison should correlate with crime.(Thought: Maybe there is more crime because people know they are less likely to be arrested. Or maybe there are less police, so the probability of being arrested is lower. I agree though, I am not entirely sure what these variables are)

```{r}
cor(crime$probarr, crime$probsen)
```

In order to determine important relationships between the crime variable and other explanatory variables, scatterplots can be analyzed. 
```{r}
for (i in 5:26)
  {
 plot(crime[,i], crime$crime,main=" ",xlab=names(crime)[i])
  title(names(crime)[i], line = -1)
}
```

Comments on scatterplots:

Police: Looks more highly correlated with crime than corr suggests. There are 1 or 2 outliers that look very influential. This is important, because we would expect that police would have a big impact on crime. Let's consider this variable for model 1 (simple regression)

Tax: Also has an influential point, so may be less correlated with crime than corr suggests. Also, cities tend to have wealthier people, with could mean more tax dollars, so this could just be a function of cities/density

ymale: Also has influential point


##Proposed Model

Important point: we are trying to use this data to understand the determinants of crime in order to generate policy suggestions that could lower crime. I believe we want to find variables we believe "cause" crime (not variables that are just correlated with crime), and can also be manipulated by local government policy in order to lower crime. e.g. things like police per capita, taxes, wages?

Model 1: One explanatory variable of key interest
Model 2: All key variables. Balancing accuracy and parsimony
Model 3: All variables

Current thoughts: use "prob" vars, mix, ymale, tax, and density as explanatory...

##CLM Assumptions

a. Linear Population Model
Nothing to reallt check here as we have specified the model in that way.

b. Random Sampling
*Not sure* Current thoughts: neighboring counties can affect one another. There can be particular pockets of crime. We don't know how accurate the records are per county (especially in non-urban areas) and we are being given averages in some case. The probabability variables call into question if there was a selection on non-zero areas only (or what methods were even used to determine these probabilities; sample porportions, bayes, etc). Would likely say this fails...

c. No Perfect Collinearity 

Verified from correlation matrix. Would need to double check once we transform and create new variables. Should also check VIF.

d. Zero-conditional mean

Check residuals vs fitted
Check covariances with residuals
Check leverage
At the least, argue for exogeneity.

If any reason to believe invalid, can argue for OLS asymptotics for consistency due to sample size.

e. Homoskedasticity
Check residuals vs fitted to see wether variance constant
Can also try Breusch-Pagan /Score-test

If any doubt, calculate robust standard errors.

f. Normality of Errors
Use Q-Q and histogram plots

##Model 2, add location and interactions?
mention assumptions that differ

##Model 3

mention assumptions that differ

##Causality


##Conclusion

